{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a169c7e",
   "metadata": {},
   "source": [
    "# Model- import - test funcions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9798af33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749735239.466791 1055125 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749735239.472717 1055125 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.environ.get('TF_ENABLE_ONEDNN_OPTS'), os.environ.get('TF_CPP_MIN_LOG_LEVEL'))\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Embedding, LSTM, Dense, AdditiveAttention, Concatenate\n",
    ")\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b67891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------- Constants --------------------\n",
    "OPERATORS = ['+', '-', '*', '/']\n",
    "IDENTIFIERS = list('abcde')\n",
    "SPECIAL_TOKENS = ['PAD', 'SOS', 'EOS']\n",
    "SYMBOLS = ['(', ')', '+', '-', '*', '/']\n",
    "VOCAB = SPECIAL_TOKENS + SYMBOLS + IDENTIFIERS + ['JUNK'] #may use junk in autoregressive generation\n",
    "\n",
    "token_to_id = {tok: i for i, tok in enumerate(VOCAB)}\n",
    "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "PAD_ID = token_to_id['PAD']\n",
    "EOS_ID = token_to_id['EOS']\n",
    "SOS_ID = token_to_id['SOS']\n",
    "\n",
    "MAX_DEPTH = 3\n",
    "MAX_LEN = 4*2**MAX_DEPTH -2 #enough to fit expressions at given depth (+ EOS)\n",
    "\n",
    "# -------------------- Expression Generation --------------------\n",
    "def generate_infix_expression(max_depth):\n",
    "    if max_depth == 0:\n",
    "        return random.choice(IDENTIFIERS)\n",
    "    elif random.random() < 0.5:\n",
    "        return generate_infix_expression(max_depth - 1)\n",
    "    else:\n",
    "        left = generate_infix_expression(max_depth - 1)\n",
    "        right = generate_infix_expression(max_depth - 1)\n",
    "        op = random.choice(OPERATORS)\n",
    "        return f'({left} {op} {right})'\n",
    "\n",
    "def tokenize(expr):\n",
    "    return [c for c in expr if c in token_to_id]\n",
    "\n",
    "def infix_to_postfix(tokens):\n",
    "    precedence = {'+': 1, '-': 1, '*': 2, '/': 2}\n",
    "    output, stack = [], []\n",
    "    for token in tokens:\n",
    "        if token in IDENTIFIERS:\n",
    "            output.append(token)\n",
    "        elif token in OPERATORS:\n",
    "            while stack and stack[-1] in OPERATORS and precedence[stack[-1]] >= precedence[token]:\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token)\n",
    "        elif token == '(':\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            while stack and stack[-1] != '(':\n",
    "                output.append(stack.pop())\n",
    "            stack.pop()\n",
    "    while stack:\n",
    "        output.append(stack.pop())\n",
    "    return output\n",
    "\n",
    "def encode(tokens, max_depth=MAX_DEPTH):\n",
    "    max_len = 4*2**max_depth -2\n",
    "    ids = [token_to_id[t] for t in tokens] + [EOS_ID]\n",
    "    return ids + [PAD_ID] * (max_len - len(ids))\n",
    "\n",
    "def decode_sequence(token_ids, id_to_token, pad_token='PAD', eos_token='EOS'):\n",
    "    \"\"\"\n",
    "    Converts a list of token IDs into a readable string by decoding tokens.\n",
    "    Stops at the first EOS token if present, and ignores PAD tokens.\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for token_id in token_ids:\n",
    "        token = id_to_token.get(token_id, '?')\n",
    "        if token == eos_token:\n",
    "            break\n",
    "        if token != pad_token:\n",
    "            tokens.append(token)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def generate_dataset(n,max_depth=MAX_DEPTH):\n",
    "    X, Y = [], []\n",
    "    for _ in range(n):\n",
    "        expr = generate_infix_expression(max_depth)\n",
    "        infix = tokenize(expr)\n",
    "        postfix = infix_to_postfix(infix)\n",
    "        X.append(encode(infix, max_depth = max_depth))\n",
    "        Y.append(encode(postfix, max_depth= max_depth))\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "#you might use the shift function for teacher-forcing\n",
    "def shift_right(seqs):\n",
    "    shifted = np.zeros_like(seqs)\n",
    "    shifted[:, 1:] = seqs[:, :-1]\n",
    "    shifted[:, 0] = SOS_ID\n",
    "    return shifted\n",
    "\n",
    "def prefix_accuracy_single(y_true, y_pred, id_to_token, eos_id=EOS_ID, verbose=False):\n",
    "    t_str = decode_sequence(y_true, id_to_token).split(' EOS')[0]\n",
    "    p_str = decode_sequence(y_pred, id_to_token).split(' EOS')[0]\n",
    "    t_tokens = t_str.strip().split()\n",
    "    p_tokens = p_str.strip().split()\n",
    "    max_len = max(len(t_tokens), len(p_tokens))\n",
    "\n",
    "    match_len = sum(x == y for x, y in zip(t_tokens, p_tokens))\n",
    "    score = match_len / max_len if max_len>0 else 0\n",
    "\n",
    "    if verbose:\n",
    "        print(\"TARGET :\", ' '.join(t_tokens))\n",
    "        print(\"PREDICT:\", ' '.join(p_tokens))\n",
    "        print(f\"PREFIX MATCH: {match_len}/{len(t_tokens)} â†’ {score:.2f}\")\n",
    "\n",
    "    return score\n",
    "\n",
    "def autoregressive_decode(encoder_input, max_depth=MAX_DEPTH):\n",
    "    max_len = 4 * 2**max_depth - 2 \n",
    "    enc_in = encoder_input[None, :] # Add batch dimension\n",
    "    enc_outs, h, c = encoder.predict(enc_in, verbose=0)\n",
    "\n",
    "    token = np.array([[SOS_ID]])\n",
    "    output_seq = []\n",
    "    for _ in range(max_len):\n",
    "        logits, h, c = decoder.predict([token, enc_outs, h, c], verbose=0)\n",
    "        # greedy decoding\n",
    "        sampled_id = np.argmax(logits[0, 0, :])\n",
    "        output_seq.append(sampled_id)\n",
    "        if sampled_id == EOS_ID:\n",
    "            break\n",
    "        token = np.array([[sampled_id]])\n",
    "    return output_seq\n",
    "\n",
    "\"\"\"\n",
    "All the adds did to the test function are to make have more or less verbose outputs\n",
    "\"\"\"\n",
    "def test(no=20,rounds=10,max_depth=MAX_DEPTH,verbose_n=0, verbose_round = True):  # added max depth and verbose\n",
    "  rscores =[]\n",
    "  for i in range(rounds):\n",
    "    if verbose_round:                                                     # added\n",
    "      print(\"================================================\")           # added\n",
    "      print(f\"round={i}\")\n",
    "      print(\"------------------------------------------------\")           # added\n",
    "    X_test, Y_test = generate_dataset(no, max_depth)                      # added max depth\n",
    "    scores = []\n",
    "    n = verbose_n                                                         # added\n",
    "    for j in range(no): \n",
    "      if n < (no+1):                                                      # added     \n",
    "        if n <= 0:                                                        # added\n",
    "          verbose = False                                                 # added\n",
    "        else:                                                             # added\n",
    "          verbose = True                                                  # added  \n",
    "        n -= 1                                                            # added\n",
    "      encoder_input=X_test[j]\n",
    "      generated = autoregressive_decode(encoder_input,max_depth=max_depth)                    # [1:] In my case no nedd to remove SOS, the function returns it whitout\n",
    "      if verbose:                                                         # added\n",
    "        print(f\"~~~~~ es number {j} ~~~~~\")                               # added\n",
    "      scores.append(prefix_accuracy_single(Y_test[j], generated, id_to_token, verbose=verbose))\n",
    "    if verbose_n > 0:                                                     # added\n",
    "      print(\"------------------------------------------------\")           # added\n",
    "    if verbose_round:                                                     # added\n",
    "      print(f\"mean scores={np.mean(scores):.4f} std={np.std(scores):.4f}\")# added\n",
    "    rscores.append(np.mean(scores))\n",
    "  mean_pref_acc = np.mean(rscores)\n",
    "  std_pref_acc = np.std(rscores)\n",
    "  print(\"================================================\")               # added\n",
    "  print(f\"FINAL SCORE = {mean_pref_acc:.4f} STD = {std_pref_acc:.4f}\")    # added\n",
    "  return mean_pref_acc,std_pref_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fb2cbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder_model(vocab_size, embed_dim, enc_units):\n",
    "    # Input of arbitrary length for infix sequence\n",
    "    enc_inputs = Input(shape=(None,), name=\"enc_inputs\")  # (batch, variable_length)\n",
    "    # Embedding\n",
    "    x = Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embed_dim,\n",
    "                  name=\"enc_embedding\")(enc_inputs)\n",
    "    # LSTM returns all hidden states and the final (h, c) states; \n",
    "    enc_outputs, state_h, state_c = LSTM(enc_units,\n",
    "                                        return_sequences=True,\n",
    "                                        return_state=True,\n",
    "                                        name=\"encoder_lstm\")(x)\n",
    "    # Model outputs encoder hidden sequence and final states\n",
    "    return Model(enc_inputs, [enc_outputs, state_h, state_c], name=\"encoder_model\")\n",
    "\n",
    "def create_decoder_model(vocab_size, embed_dim, dec_units):\n",
    "    # Decoder inputs (token IDs) of variable length for teacher forcing\n",
    "    dec_inputs = Input(shape=(None,), name=\"dec_inputs\")  # (batch, variable_length)\n",
    "    # Encoder outputs and states come from encoder\n",
    "    enc_outputs = Input(shape=(None, dec_units), name=\"enc_outputs\")  # (batch, enc_len, units)\n",
    "    state_h_in  = Input(shape=(dec_units,), name=\"h_in\")\n",
    "    state_c_in  = Input(shape=(dec_units,), name=\"c_in\")\n",
    "\n",
    "    # Embedding of decoder input\n",
    "    x = Embedding(input_dim=vocab_size,\n",
    "                  output_dim=embed_dim,\n",
    "                  name=\"dec_embedding\")(dec_inputs)\n",
    "    # Decoder LSTM consumes masked embeddings and initial states\n",
    "    dec_outputs, state_h_out, state_c_out = LSTM(dec_units,\n",
    "                                                return_sequences=True,\n",
    "                                                return_state=True,\n",
    "                                                name=\"decoder_lstm\")(x,\n",
    "                                                                   initial_state=[state_h_in, state_c_in])\n",
    "    # It aligns each decoder timestep to relevant encoder hidden states\n",
    "    context = AdditiveAttention(name=\"attention\")([dec_outputs, enc_outputs])\n",
    "    # Concatenate context vector with decoder outputs for richer representation\n",
    "    concat = Concatenate(axis=-1, name=\"concat\")([dec_outputs, context])\n",
    "\n",
    "    # Final projection to vocabulary distribution (softmax)\n",
    "    logits = Dense(vocab_size, activation=\"softmax\", name=\"vocab_dist\")(concat)\n",
    "\n",
    "    # Return logits and new states for inference\n",
    "    return Model(\n",
    "        [dec_inputs, enc_outputs, state_h_in, state_c_in],\n",
    "        [logits, state_h_out, state_c_out],\n",
    "        name=\"decoder_model\"\n",
    "    )\n",
    "\n",
    "# Combined Seq2Seq Model: for training with teacher forcing\n",
    "def create_seq2seq_model(encoder, decoder):\n",
    "    # Encoder input and decoder input\n",
    "    enc_in = encoder.input       # (batch, variable_length)\n",
    "    dec_in = decoder.input[0]    # (batch, variable_length)\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    enc_outs, h, c = encoder(enc_in)\n",
    "    # Forward pass through decoder using teacher-forcing inputs and encoder outputs/states\n",
    "    logits, _, _ = decoder([dec_in, enc_outs, h, c])\n",
    "\n",
    "    # Full model maps [enc_in, dec_in] -> logits\n",
    "    return Model([enc_in, dec_in], logits, name=\"seq2seq_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131013fb",
   "metadata": {},
   "source": [
    "# test 1 (2.8M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cedf9d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1749735241.127762 1055125 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "EMBED_DIM  = 32\n",
    "ENC_UNITS = 256\n",
    "DEC_UNITS = 256\n",
    "\n",
    "encoder = create_encoder_model(VOCAB_SIZE, EMBED_DIM, ENC_UNITS)\n",
    "decoder = create_decoder_model(VOCAB_SIZE, EMBED_DIM, DEC_UNITS)\n",
    "seq2seq = create_seq2seq_model(encoder, decoder)\n",
    "PATH = \"/home/pp26/ml/progettoDL/report/model/model_finale_1.8M/seq2seq_attention_model_v1.check_16.weights.h5\"\n",
    "\n",
    "seq2seq.load_weights(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be8e9054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 0.9996 STD = 0.0012\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    print(\"Testing autoregressive decoding with 20 samples...\")\n",
    "    mean_pref_acc, std_pref_acc = test(verbose_n=0, verbose_round=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a4908b",
   "metadata": {},
   "source": [
    "# test 2 (500m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe591726",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM  = 32\n",
    "ENC_UNITS = 128\n",
    "DEC_UNITS = 128\n",
    "\n",
    "encoder = create_encoder_model(VOCAB_SIZE, EMBED_DIM, ENC_UNITS)\n",
    "decoder = create_decoder_model(VOCAB_SIZE, EMBED_DIM, DEC_UNITS)\n",
    "seq2seq = create_seq2seq_model(encoder, decoder)\n",
    "PATH = \"/home/pp26/ml/progettoDL/report/model/model_finale_500m/seq2seq_attention_model_v2.1.weights.h5\"\n",
    "\n",
    "seq2seq.load_weights(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca01bfcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    print(\"Testing autoregressive decoding with 20 samples...\")\n",
    "    mean_pref_acc, std_pref_acc = test(verbose_n=0, verbose_round=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bf351a",
   "metadata": {},
   "source": [
    "# test 3  (132m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f169d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1\n",
    "EMBED_DIM  = 16 #<-------------------------------\n",
    "ENC_UNITS = 64\n",
    "DEC_UNITS = 64\n",
    "\n",
    "encoder = create_encoder_model(VOCAB_SIZE, EMBED_DIM, ENC_UNITS)\n",
    "decoder = create_decoder_model(VOCAB_SIZE, EMBED_DIM, DEC_UNITS)\n",
    "seq2seq = create_seq2seq_model(encoder, decoder)\n",
    "PATH = \"/home/pp26/ml/progettoDL/report/model/model_v4.0/seq2seq_attention_model_v4.0.weights.h5\"\n",
    "\n",
    "seq2seq.load_weights(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38569a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 0.9982 STD = 0.0055\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    print(\"Testing autoregressive decoding with 20 samples...\")\n",
    "    mean_pref_acc, std_pref_acc = test(verbose_n=0, verbose_round=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d368e7d2",
   "metadata": {},
   "source": [
    "# test 4 (42m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "faa9230b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test 1\n",
    "EMBED_DIM  = 16 #<-------------------------------\n",
    "ENC_UNITS = 32\n",
    "DEC_UNITS = 32\n",
    "\n",
    "encoder = create_encoder_model(VOCAB_SIZE, EMBED_DIM, ENC_UNITS)\n",
    "decoder = create_decoder_model(VOCAB_SIZE, EMBED_DIM, DEC_UNITS)\n",
    "seq2seq = create_seq2seq_model(encoder, decoder)\n",
    "PATH = \"/home/pp26/ml/progettoDL/report/model/model_v4.1/seq2seq_attention_model_v4.1.weights.h5\"\n",
    "\n",
    "seq2seq.load_weights(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9f224e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 0.9992 STD = 0.0023\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n",
      "Testing autoregressive decoding with 20 samples...\n",
      "================================================\n",
      "FINAL SCORE = 1.0000 STD = 0.0000\n"
     ]
    }
   ],
   "source": [
    "for _ in range(20):\n",
    "    print(\"Testing autoregressive decoding with 20 samples...\")\n",
    "    mean_pref_acc, std_pref_acc = test(verbose_n=0, verbose_round=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
