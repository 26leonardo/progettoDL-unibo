{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1160f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2e7350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impostazione del seed globale a 42 per riproducibilità...\n"
     ]
    }
   ],
   "source": [
    "def set_global_seed(seed=42):\n",
    "    print(f\"Impostazione del seed globale a {seed} per riproducibilità...\")\n",
    "\n",
    "    # Seed Python\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Seed NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Seed TensorFlow\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Impostazioni di determinismo per TensorFlow\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # solo su TF >= 2.8\n",
    "\n",
    "\n",
    "# ESEMPIO DI USO\n",
    "SEED = 42\n",
    "set_global_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9d5e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "#  COSTANTI E DIZIONARI VOCABOLARIO\n",
    "# =============================================================================\n",
    "OPERATORS      = ['+', '-', '*', '/']\n",
    "IDENTIFIERS    = list('abcde')\n",
    "SPECIAL_TOKENS = ['PAD', 'SOS', 'EOS']\n",
    "SYMBOLS        = ['(', ')', '+', '-', '*', '/']\n",
    "\n",
    "VOCAB = SPECIAL_TOKENS + SYMBOLS + IDENTIFIERS + ['JUNK']\n",
    "token_to_id = {tok: i for i, tok in enumerate(VOCAB)}\n",
    "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "\n",
    "VOCAB_SIZE = len(VOCAB)       # ≈ 15\n",
    "PAD_ID     = token_to_id['PAD']\n",
    "SOS_ID     = token_to_id['SOS']\n",
    "EOS_ID     = token_to_id['EOS']\n",
    "\n",
    "MAX_DEPTH = 3\n",
    "MAX_LEN   = 4 * (2 ** MAX_DEPTH) - 2  # = 30\n",
    "\n",
    "# =============================================================================\n",
    "#  FUNZIONI PER LA GENERAZIONE DEL DATASET\n",
    "# =============================================================================\n",
    "def generate_infix_expression(max_depth: int) -> str:\n",
    "    \"\"\"\n",
    "    Genera un'espressione infix pienamente parentesizzata fino a profondità max_depth.\n",
    "    Profondità 0: estrae casualmente da IDENTIFIERS. Altrimenti, con 50% ricorre su un sotto-\n",
    "    espressione (diminuendo depth), altrimenti concatena (left op right) con nuove ricorsioni.\n",
    "    \"\"\"\n",
    "    if max_depth == 0:\n",
    "        return random.choice(IDENTIFIERS)\n",
    "    elif random.random() < 0.5:\n",
    "        return generate_infix_expression(max_depth - 1)\n",
    "    else:\n",
    "        left = generate_infix_expression(max_depth - 1)\n",
    "        right = generate_infix_expression(max_depth - 1)\n",
    "        op = random.choice(OPERATORS)\n",
    "        # il formato contiene spazi per agevolare tokenize()\n",
    "        return f'({left} {op} {right})'\n",
    "\n",
    "def tokenize(expr: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Filtra i caratteri dell'espressione che compaiono in token_to_id.\n",
    "    Esempio: \"(a + (b * c))\" -> ['(', 'a', '+', '(', 'b', '*', 'c', ')', ')']\n",
    "    \"\"\"\n",
    "    return [c for c in expr if c in token_to_id]\n",
    "\n",
    "def infix_to_postfix(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Converte la lista di token infix in una lista di token postfix usando uno stack.\n",
    "    Precedenze: +,- (1); *,/ (2). Non si gestiscono associatività perché tutto è già parentesizzato.\n",
    "    \"\"\"\n",
    "    precedence = {'+': 1, '-': 1, '*': 2, '/': 2}\n",
    "    output, stack = [], []\n",
    "    for token in tokens:\n",
    "        if token in IDENTIFIERS:\n",
    "            output.append(token)\n",
    "        elif token in OPERATORS:\n",
    "            # pop finché in cima a stack c'è operatore con precedenza >=\n",
    "            while stack and stack[-1] in OPERATORS and precedence[stack[-1]] >= precedence[token]:\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token)\n",
    "        elif token == '(':\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            # pop finché non trovo \"(\"\n",
    "            while stack and stack[-1] != '(':\n",
    "                output.append(stack.pop())\n",
    "            stack.pop()  # rimuovo \"(\"\n",
    "    # svuoto lo stack\n",
    "    while stack:\n",
    "        output.append(stack.pop())\n",
    "    return output\n",
    "\n",
    "def encode(tokens: list[str], max_len: int = MAX_LEN) -> list[int]:\n",
    "    \"\"\"\n",
    "    Converte lista di token in ID, aggiunge EOS e padding PAD fino a max_len.\n",
    "    \"\"\"\n",
    "    ids = [token_to_id[t] for t in tokens] + [EOS_ID]\n",
    "    assert len(ids) <= max_len, f\"Lunghezza {len(ids)} > {max_len}\"\n",
    "    return ids + [PAD_ID] * (max_len - len(ids))\n",
    "\n",
    "def decode_sequence(token_ids: list[int]) -> str:\n",
    "    \"\"\"\n",
    "    Converte lista di ID in stringa tokenizzata, fermandosi a EOS e ignorando PAD.\n",
    "    Esempio: [ 'c','b','*','e','*','d','b','/','+','EOS',PAD,.. ] -> \"c b * e * d b / +\"\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for token_id in token_ids:\n",
    "        tok = id_to_token.get(token_id, '?')\n",
    "        if tok == 'EOS':\n",
    "            break\n",
    "        if tok != 'PAD':\n",
    "            tokens.append(tok)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def shift_right(seqs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sposta a destra ogni sequenza target per il teacher forcing, \n",
    "    inserendo SOS all'inizio. Shape in input/output: [batch, MAX_LEN].\n",
    "    \"\"\"\n",
    "    shifted = np.zeros_like(seqs)\n",
    "    shifted[:, 1:] = seqs[:, :-1]\n",
    "    shifted[:, 0] = SOS_ID\n",
    "    return shifted\n",
    "\n",
    "def generate_dataset(n: int, max_depth: int = MAX_DEPTH) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Genera n espressioni random, ritorna due array:\n",
    "      - X: [n, MAX_LEN] con ID di infix codificato + EOS + PAD\n",
    "      - Y: [n, MAX_LEN] con ID di postfix codificato + EOS + PAD\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for _ in range(n):\n",
    "        expr = generate_infix_expression(max_depth)\n",
    "        infix = tokenize(expr)\n",
    "        postfix = infix_to_postfix(infix)\n",
    "        X.append(encode(infix))\n",
    "        Y.append(encode(postfix))\n",
    "    return np.array(X, dtype=np.int32), np.array(Y, dtype=np.int32)\n",
    "\n",
    "# =============================================================================\n",
    "#  DEFINIZIONE DEL MECCANISMO DI ATTENTION (Bahdanau)\n",
    "# =============================================================================\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementazione di Bahdanau (additive) attention:\n",
    "      score = v^T tanh(W1 · values + W2 · query)\n",
    "      alpha = softmax(score, axis=1)\n",
    "      context = Σ_s alpha_s * values_s\n",
    "    Dove:\n",
    "      - values: intera sequenza di hidden states dell'encoder (shape [batch, seq_len, enc_units])\n",
    "      - query: hidden state corrente del decoder (shape [batch, dec_units])\n",
    "    Ritorna:\n",
    "      - context_vector: [batch, enc_units]\n",
    "      - attention_weights: [batch, seq_len, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, units: int):\n",
    "        super().__init__()\n",
    "        # W1: proietta encoder_outputs da (enc_units) a (units)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        # W2: proietta decoder_hidden (dec_units) a (units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        # V: proietta la somma tanh(W1+W2) a un punteggio scalare\n",
    "        self.V  = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query: tf.Tensor, values: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        query: [batch, dec_units]\n",
    "        values: [batch, seq_len, enc_units]\n",
    "        \"\"\"\n",
    "        # Aggiungi time axis per query: [batch, 1, dec_units]\n",
    "        hidden_with_time_axis = tf.expand_dims(query, axis=1)\n",
    "\n",
    "        # Calcolo score: shape -> [batch, seq_len, 1]\n",
    "        #   score = V(tanh(W1(values) + W2(query)))\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(values) + self.W2(hidden_with_time_axis)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # attention_weights: softmax su axis=1 (su seq_len)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)  # [batch, seq_len, 1]\n",
    "\n",
    "        # context_vector = Σ_i α_i * values_i\n",
    "        context_vector = attention_weights * values  # broadcasting [batch, seq_len, enc_units]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)  # [batch, enc_units]\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "# =============================================================================\n",
    "#  DEFINIZIONE DEL MODELLO SEQ2SEQ CON ATTENTION\n",
    "# =============================================================================\n",
    "\n",
    "# Iperparametri\n",
    "EMB_DIM    = 64      # dimensione embedding (sia encoder che decoder)\n",
    "ENC_UNITS  = 128     # numero di celle in encoder LSTM\n",
    "DEC_UNITS  = 128     # numero di celle in decoder LSTM (coerente con ENC_UNITS)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# --- 1) Definizione degli Input Layers ---\n",
    "# encoder_input: sequenze infix (shape [batch, MAX_LEN])\n",
    "encoder_inputs = Input(shape=(MAX_LEN,), name='encoder_inputs')\n",
    "\n",
    "# decoder_input: sequenze shiftate (shape [batch, MAX_LEN])\n",
    "decoder_inputs = Input(shape=(MAX_LEN,), name='decoder_inputs')\n",
    "\n",
    "# --- 2) Embedding Layers (encoder e decoder) ---\n",
    "encoder_embedding = Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=EMB_DIM,\n",
    "    embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=SEED),\n",
    "    mask_zero=True,                   # per ignorare PAD\n",
    "    name='encoder_embedding'\n",
    ")(encoder_inputs)  # shape -> [batch, MAX_LEN, EMB_DIM]\n",
    "\n",
    "decoder_embedding = Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=EMB_DIM,\n",
    "    embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=SEED+1),\n",
    "    mask_zero=True,\n",
    "    name='decoder_embedding'\n",
    ")(decoder_inputs)  # shape -> [batch, MAX_LEN, EMB_DIM]\n",
    "\n",
    "# --- 3) Encoder LSTM (return_sequences + return_state) ---\n",
    "# Usando un LSTM unidirezionale per semplicità; si potrebbero usare Bidirectional\n",
    "encoder_lstm = LSTM(\n",
    "    ENC_UNITS,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer=tf.keras.initializers.GlorotUniform(seed=SEED+2),\n",
    "    name='encoder_lstm'\n",
    ")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "# encoder_outputs: [batch, MAX_LEN, ENC_UNITS]\n",
    "# state_h, state_c: entrambi [batch, ENC_UNITS]\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# --- 4) Decoder LSTM (return_sequences=True per generare tutta la sequenza in training) ---\n",
    "decoder_lstm = LSTM(\n",
    "    DEC_UNITS,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer=tf.keras.initializers.GlorotUniform(seed=SEED+3),\n",
    "    name='decoder_lstm'\n",
    ")\n",
    "# Forniamo come initial_state lo stato finale dell'encoder\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# decoder_outputs: [batch, MAX_LEN, DEC_UNITS]\n",
    "\n",
    "# --- 5) Meccanismo di Attention Bahdanau (appliicato ad ogni passo) ---\n",
    "# Creiamo una singola istanza di BahdanauAttention\n",
    "attention_layer = BahdanauAttention(units=DEC_UNITS)\n",
    "\n",
    "# Vogliamo applicare l’attenzione passo per passo su tutta la lunghezza di decoder_outputs.\n",
    "# Tuttavia, Keras non supporta direttamente un loop interno in Functional API.\n",
    "# Quindi possiamo “ricostruire” la parte finale come:\n",
    "#   per ogni passo t in MAX_LEN:\n",
    "#       h_t = decoder_outputs[:, t, :]  # shape [batch, DEC_UNITS]\n",
    "#       context_t, attn_weights_t = attention_layer(h_t, encoder_outputs)\n",
    "#       concat_t = concat([context_t, h_t], axis=-1)  # [batch, DEC_UNITS+ENC_UNITS]\n",
    "#       out_t = Dense(VOCAB_SIZE, activation='softmax')(concat_t)\n",
    "# E poi raccogliere tutti i out_t in una sequenza temporale.\n",
    "#\n",
    "# Con Keras Functional, si può invece fare un _TimeDistributed_ di un “mini-decoder” che\n",
    "# unisce attention e Dense. Tuttavia, per chiarezza, implementiamo un loop via tf.keras.layers.Lambda.\n",
    "\n",
    "def apply_attention_step(args):\n",
    "    \"\"\"\n",
    "    Funzione di utilità per calcolare attenzione e output softmax ad un singolo passo.\n",
    "    args = (decoder_hidden_t, encoder_outputs)\n",
    "      - decoder_hidden_t: [batch, DEC_UNITS]\n",
    "      - encoder_outputs: [batch, MAX_LEN, ENC_UNITS]\n",
    "    Ritorna:\n",
    "      - output_t: [batch, VOCAB_SIZE]\n",
    "    \"\"\"\n",
    "    decoder_hidden_t, encoder_outs = args\n",
    "    # 1) calcolo context vector e att. weights\n",
    "    context_vector, _ = attention_layer(query=decoder_hidden_t, values=encoder_outs)\n",
    "    # 2) concat: [batch, ENC_UNITS] + [batch, DEC_UNITS] = [batch, ENC_UNITS+DEC_UNITS]\n",
    "    concat_vector = K.concatenate([context_vector, decoder_hidden_t], axis=-1)\n",
    "    # 3) Dense -> VOCAB_SIZE con softmax\n",
    "    output_t = Dense(\n",
    "        VOCAB_SIZE,\n",
    "        activation='softmax',\n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED+4)\n",
    "    )(concat_vector)\n",
    "    return output_t  # shape [batch, VOCAB_SIZE]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e34ff8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'split_decoder_outputs' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_0' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_1' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_2' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_3' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_4' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_5' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_6' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_7' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_8' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_9' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_10' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_11' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_12' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_13' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_14' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_15' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_16' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_17' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_18' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_19' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_20' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_21' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_22' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_23' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_24' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_25' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_26' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_27' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_28' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/layer.py:932: UserWarning: Layer 'attention_step_29' (of type Lambda) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     all_decoder_outputs\u001b[38;5;241m.\u001b[39mappend(out_t)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Ricompattiamo la lista in [batch, MAX_LEN, VOCAB_SIZE]\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m decoder_softmax_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_decoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# --- 6) Modello Keras Finale (Training) ---\u001b[39;00m\n\u001b[1;32m     35\u001b[0m model \u001b[38;5;241m=\u001b[39m Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/backend/common/keras_tensor.py:138\u001b[0m, in \u001b[0;36mKerasTensor.__tf_tensor__\u001b[0;34m(self, dtype, name)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__tf_tensor__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor cannot be used as input to a TensorFlow function. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA KerasTensor is a symbolic placeholder for a shape and dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mused when constructing Keras Functional models \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor Keras Functions. You can only use it as input to a Keras layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor a Keras operation (from the namespaces `keras.layers` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    144\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand `keras.operations`). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are likely doing something like:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    146\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = Input(...)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    149\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf_fn(x)  # Invalid.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    150\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    151\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat you should do instead is wrap `tf_fn` in a layer:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    153\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass MyLayer(Layer):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    154\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    def call(self, x):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m        return tf_fn(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx = MyLayer()(x)\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    158\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n\n```\nx = Input(...)\n...\ntf_fn(x)  # Invalid.\n```\n\nWhat you should do instead is wrap `tf_fn` in a layer:\n\n```\nclass MyLayer(Layer):\n    def call(self, x):\n        return tf_fn(x)\n\nx = MyLayer()(x)\n```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras.layers import Lambda\n",
    "\n",
    "# Questo crea un layer che, nel grafo di Keras, applicherà tf.split\n",
    "dec_hidden_seq_list = Lambda(\n",
    "    lambda x: tf.split(x, num_or_size_splits=MAX_LEN, axis=1),\n",
    "    name='split_decoder_outputs'\n",
    ")(decoder_outputs)\n",
    "\n",
    "# Nota: dec_hidden_seq_list sarà un TUPLE di MAX_LEN KerasTensor di forma [batch, 1, DEC_UNITS].\n",
    "# Poi, per ridurre la dimensione 1, usi ancora una Lambda o tf.keras.layers.Reshape.\n",
    "dec_hidden_seq = [\n",
    "    Lambda(lambda y: tf.squeeze(y, axis=1), name=f'squeeze_step_{t}')(dec_hidden_seq_list[t])\n",
    "    for t in range(MAX_LEN)\n",
    "]\n",
    "\n",
    "# # Split decoder_outputs lungo l’asse temporale: lista di MAX_LEN tensori [batch, DEC_UNITS]\n",
    "# dec_hidden_seq = tf.split(decoder_outputs, num_or_size_splits=MAX_LEN, axis=1)\n",
    "# # Ogni dec_hidden_seq[i] ha shape [batch, 1, DEC_UNITS], vogliamo ridurlo a [batch, DEC_UNITS]\n",
    "# dec_hidden_seq = [K.squeeze(x, axis=1) for x in dec_hidden_seq]\n",
    "\n",
    "# Per ognuno dei MAX_LEN \"step\", applichiamo apply_attention_step:\n",
    "all_decoder_outputs = []\n",
    "for t in range(MAX_LEN):\n",
    "    # dec_hidden_seq[t]: [batch, DEC_UNITS]\n",
    "    out_t = tf.keras.layers.Lambda(\n",
    "        apply_attention_step,\n",
    "        name=f'attention_step_{t}'\n",
    "    )((dec_hidden_seq[t], encoder_outputs))  # out_t: [batch, VOCAB_SIZE]\n",
    "    all_decoder_outputs.append(out_t)\n",
    "\n",
    "# Ricompattiamo la lista in [batch, MAX_LEN, VOCAB_SIZE]\n",
    "decoder_softmax_outputs = tf.stack(all_decoder_outputs, axis=1)\n",
    "\n",
    "# --- 6) Modello Keras Finale (Training) ---\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "# Compiliamo con loss sparse_categorical_crossentropy (per ID target) e optimizer Adam\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()  # possibilità di verificare il numero totale di parametri (<2M) :contentReference[oaicite:4]{index=4}\n",
    "\n",
    "# =============================================================================\n",
    "#  PREPARAZIONE DEI DATI E TRAINING\n",
    "# =============================================================================\n",
    "# Generiamo un dataset fisso (ad esempio 100k esempi) o usiamo un generatore on-the-fly.\n",
    "N_SAMPLES = 100_000\n",
    "X_data, Y_data = generate_dataset(N_SAMPLES, MAX_DEPTH)\n",
    "\n",
    "# Input al decoder per teacher forcing\n",
    "decoder_input_data = shift_right(Y_data)\n",
    "\n",
    "# Poiché la loss è 'sparse_categorical_crossentropy', ci aspettiamo come target gli ID\n",
    "# shape Y_data: [N_SAMPLES, MAX_LEN], ma il softmax output è [batch, MAX_LEN, VOCAB_SIZE],\n",
    "# quindi va bene lasciare Y_data come è (Keras farà il broadcasting).\n",
    "\n",
    "# Divido in train/val (es. 90% train, 10% val)\n",
    "indices = np.arange(N_SAMPLES)\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.9 * N_SAMPLES)\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "X_train, X_val = X_data[train_idx], X_data[val_idx]\n",
    "dec_in_train, dec_in_val = decoder_input_data[train_idx], decoder_input_data[val_idx]\n",
    "Y_train, Y_val = Y_data[train_idx], Y_data[val_idx]\n",
    "\n",
    "# Eseguo il training (ad esempio 20 epoche)\n",
    "EPOCHS = 20\n",
    "history = model.fit(\n",
    "    [X_train, dec_in_train],        # encoder input + decoder input\n",
    "    np.expand_dims(Y_train, -1),     # serve come [batch, MAX_LEN, 1] per sparse_cce\n",
    "    validation_data=(\n",
    "        [X_val, dec_in_val],\n",
    "        np.expand_dims(Y_val, -1)\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n",
    "\n",
    "# =============================================================================\n",
    "#  INFERENZA AUTOREGRESSIVA (SENZA BEAM SEARCH)\n",
    "# =============================================================================\n",
    "# Per l'inferenza, dobbiamo definire i modelli separate encoder_model e decoder_model,\n",
    "# così da far girare un ciclo passo-passo (greedy) finché non usciamo con EOS.\n",
    "\n",
    "# ==== 1) Modello encoder per inferenza (input -> hidden states) ====\n",
    "encoder_model_inf = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# ==== 2) Modello decoder per inferenza ====\n",
    "# Input placeholder per hidden state e cell state del decoder (prendono ENC_UNITS)\n",
    "decoder_state_input_h = Input(shape=(DEC_UNITS,), name='dec_state_h')\n",
    "decoder_state_input_c = Input(shape=(DEC_UNITS,), name='dec_state_c')\n",
    "# Input placeholder per encoder_outputs (per l’attenzione) [batch, MAX_LEN, ENC_UNITS]\n",
    "encoder_outputs_inf = Input(shape=(MAX_LEN, ENC_UNITS), name='enc_outputs_inf')\n",
    "\n",
    "# Embedding del token di input corrente (shape [batch=1, 1]) in inference\n",
    "dec_single_input = Input(shape=(1,), name='dec_single_input')  # st(1) per inferenza\n",
    "dec_single_emb = decoder_embedding.layer(dec_single_input)      # usiamo lo stesso embedding\n",
    "\n",
    "# LSTM del decoder ad un singolo passo, con hidden iniziali\n",
    "dec_lstm_inf = decoder_lstm  # usiamo lo stesso layer addestrato\n",
    "# Riduci dec_single_emb (che è [batch, 1, EMB_DIM]) in [batch, 1, EMB_DIM],\n",
    "# già corretto shape.\n",
    "dec_outputs, dec_state_h_new, dec_state_c_new = dec_lstm_inf(\n",
    "    dec_single_emb, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")  # dec_outputs: [batch, 1, DEC_UNITS]\n",
    "\n",
    "# Rimuovo la dimensione temporale: [batch, DEC_UNITS]\n",
    "dec_hidden_t = K.squeeze(dec_outputs, axis=1)\n",
    "\n",
    "# Applico attention: query=dec_hidden_t, values=encoder_outputs_inf\n",
    "context_vector_inf, _ = attention_layer(dec_hidden_t, encoder_outputs_inf)\n",
    "concat_vector_inf = K.concatenate([context_vector_inf, dec_hidden_t], axis=-1)\n",
    "# Softmax output\n",
    "dec_pred = Dense(\n",
    "    VOCAB_SIZE,\n",
    "    activation='softmax',\n",
    "    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED+4)\n",
    ")(concat_vector_inf)  # [batch, VOCAB_SIZE]\n",
    "\n",
    "# Definisco il modello decoder_inferenza che, dato:\n",
    "#  - dec_single_input: token corrente [batch,1]\n",
    "#  - encoder_outputs_inf: [batch,MAX_LEN,ENC_UNITS]\n",
    "#  - dec_state_input_h, dec_state_input_c: [batch,DEC_UNITS]\n",
    "# produce:\n",
    "#  - dec_pred: [batch, VOCAB_SIZE] predisposto all'argmax\n",
    "#  - dec_state_h_new, dec_state_c_new (nuovi state)\n",
    "decoder_model_inf = Model(\n",
    "    [dec_single_input, encoder_outputs_inf, decoder_state_input_h, decoder_state_input_c],\n",
    "    [dec_pred, dec_state_h_new, dec_state_c_new]\n",
    ")\n",
    "\n",
    "# ==== 3) Funzione di decodifica greedy in inferenza ====\n",
    "def decode_sequence_inference(input_seq: np.ndarray) -> list[int]:\n",
    "    \"\"\"\n",
    "    Data una singola sequenza infix (shape [1, MAX_LEN]), genera la corrispondente \n",
    "    postfix autoregressivamente fino a EOS o lunghezza MAX_LEN. Ritorna lista di ID.\n",
    "    \"\"\"\n",
    "    # 1) Ottengo encoder_outputs e stati iniziali\n",
    "    enc_outs, enc_h, enc_c = encoder_model_inf.predict(input_seq)\n",
    "    # inizializzo decoder con SOS\n",
    "    target_seq = np.array([[SOS_ID]], dtype=np.int32)\n",
    "    dec_h, dec_c = enc_h, enc_c\n",
    "\n",
    "    output_ids = []\n",
    "    for _ in range(MAX_LEN):\n",
    "        # 2) Chiama decoder one-step\n",
    "        preds, dec_h, dec_c = decoder_model_inf.predict([target_seq, enc_outs, dec_h, dec_c])\n",
    "        # preds: [1, VOCAB_SIZE]\n",
    "        sampled_id = np.argmax(preds[0])\n",
    "        # Se arrivo a EOS, fermo\n",
    "        if sampled_id == EOS_ID:\n",
    "            break\n",
    "        output_ids.append(sampled_id)\n",
    "        # preparo input per passo successivo: shape [1,1]\n",
    "        target_seq = np.array([[sampled_id]], dtype=np.int32)\n",
    "\n",
    "    return output_ids\n",
    "\n",
    "# =============================================================================\n",
    "#  METRICA Prefix Accuracy\n",
    "# =============================================================================\n",
    "def prefix_accuracy(y_true_ids: list[int], y_pred_ids: list[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calcola la prefix_accuracy tra due sequence di ID (senza pad).\n",
    "    Ritorna lunghezza del prefisso identico / max(len(y_true), len(y_pred)).\n",
    "    \"\"\"\n",
    "    # Tronco alla comparsa di EOS (se presente) – ma qui assume che le sequenze non contengano EOS        \n",
    "    # Confronto elemento per elemento\n",
    "    match_len = 0\n",
    "    L_true = len(y_true_ids)\n",
    "    L_pred = len(y_pred_ids)\n",
    "    L_max = max(L_true, L_pred)\n",
    "    for i in range(min(L_true, L_pred)):\n",
    "        if y_true_ids[i] == y_pred_ids[i]:\n",
    "            match_len += 1\n",
    "        else:\n",
    "            break\n",
    "    return match_len / L_max if L_max > 0 else 1.0\n",
    "\n",
    "# Esempio di utilizzo:\n",
    "i = np.random.randint(len(X_val))\n",
    "input_seq = X_val[i : i + 1]  # shape [1, MAX_LEN]\n",
    "true_postfix = Y_val[i]\n",
    "pred_ids = decode_sequence_inference(input_seq)\n",
    "print(\"Infix   : \", decode_sequence(input_seq[0]))\n",
    "print(\"TP (true postfix) : \", decode_sequence(true_postfix))\n",
    "print(\"Pred    : \", decode_sequence(pred_ids))\n",
    "print(\"PrefixAcc        : \", prefix_accuracy(\n",
    "    # rimuovo pad/EOS in true_postfix\n",
    "    [tok for tok in true_postfix if tok not in (PAD_ID, EOS_ID)],\n",
    "    pred_ids\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
