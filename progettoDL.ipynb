{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c11698",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1160f19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2e7350b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Impostazione del seed globale a 42 per riproducibilità...\n"
     ]
    }
   ],
   "source": [
    "def set_global_seed(seed=42):\n",
    "    print(f\"Impostazione del seed globale a {seed} per riproducibilità...\")\n",
    "\n",
    "    # Seed Python\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Seed NumPy\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # Seed TensorFlow\n",
    "    tf.keras.utils.set_random_seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "    # Set determinism for TensorFlow\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'  # solo su TF >= 2.8\n",
    "\n",
    "SEED = 42\n",
    "set_global_seed(SEED)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a52c8",
   "metadata": {},
   "source": [
    "# Constants and Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb4567b",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPERATORS      = ['+', '-', '*', '/']\n",
    "IDENTIFIERS    = list('abcde')\n",
    "SPECIAL_TOKENS = ['PAD', 'SOS', 'EOS']\n",
    "SYMBOLS        = ['(', ')', '+', '-', '*', '/']\n",
    "\n",
    "VOCAB = SPECIAL_TOKENS + SYMBOLS + IDENTIFIERS + ['JUNK']\n",
    "token_to_id = {tok: i for i, tok in enumerate(VOCAB)}\n",
    "id_to_token = {i: tok for tok, i in token_to_id.items()}\n",
    "\n",
    "VOCAB_SIZE = len(VOCAB)       # ≈ 15\n",
    "PAD_ID     = token_to_id['PAD']\n",
    "SOS_ID     = token_to_id['SOS']\n",
    "EOS_ID     = token_to_id['EOS']\n",
    "\n",
    "MAX_DEPTH = 3\n",
    "MAX_LEN   = 4 * (2 ** MAX_DEPTH) - 2  # = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4476e",
   "metadata": {},
   "source": [
    "# Functions for generating dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ba973fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_infix_expression(max_depth: int) -> str:\n",
    "    \"\"\"\n",
    "    Genera un'espressione infix pienamente parentesizzata fino a profondità max_depth.\n",
    "    Profondità 0: estrae casualmente da IDENTIFIERS. Altrimenti, con 50% ricorre su un sotto-\n",
    "    espressione (diminuendo depth), altrimenti concatena (left op right) con nuove ricorsioni.\n",
    "    \"\"\"\n",
    "    if max_depth == 0:\n",
    "        return random.choice(IDENTIFIERS)\n",
    "    elif random.random() < 0.5:\n",
    "        return generate_infix_expression(max_depth - 1)\n",
    "    else:\n",
    "        left = generate_infix_expression(max_depth - 1)\n",
    "        right = generate_infix_expression(max_depth - 1)\n",
    "        op = random.choice(OPERATORS)\n",
    "        # il formato contiene spazi per agevolare tokenize()\n",
    "        return f'({left} {op} {right})'\n",
    "\n",
    "def tokenize(expr: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Filtra i caratteri dell'espressione che compaiono in token_to_id.\n",
    "    Esempio: \"(a + (b * c))\" -> ['(', 'a', '+', '(', 'b', '*', 'c', ')', ')']\n",
    "    \"\"\"\n",
    "    return [c for c in expr if c in token_to_id]\n",
    "\n",
    "def infix_to_postfix(tokens: list[str]) -> list[str]:\n",
    "    \"\"\"\n",
    "    Converte la lista di token infix in una lista di token postfix usando uno stack.\n",
    "    Precedenze: +,- (1); *,/ (2). Non si gestiscono associatività perché tutto è già parentesizzato.\n",
    "    \"\"\"\n",
    "    precedence = {'+': 1, '-': 1, '*': 2, '/': 2}\n",
    "    output, stack = [], []\n",
    "    for token in tokens:\n",
    "        if token in IDENTIFIERS:\n",
    "            output.append(token)\n",
    "        elif token in OPERATORS:\n",
    "            # pop finché in cima a stack c'è operatore con precedenza >=\n",
    "            while stack and stack[-1] in OPERATORS and precedence[stack[-1]] >= precedence[token]:\n",
    "                output.append(stack.pop())\n",
    "            stack.append(token)\n",
    "        elif token == '(':\n",
    "            stack.append(token)\n",
    "        elif token == ')':\n",
    "            # pop finché non trovo \"(\"\n",
    "            while stack and stack[-1] != '(':\n",
    "                output.append(stack.pop())\n",
    "            stack.pop()  # rimuovo \"(\"\n",
    "    # svuoto lo stack\n",
    "    while stack:\n",
    "        output.append(stack.pop())\n",
    "    return output\n",
    "\n",
    "def encode(tokens: list[str], max_len: int = MAX_LEN) -> list[int]:\n",
    "    \"\"\"\n",
    "    Converte lista di token in ID, aggiunge EOS e padding PAD fino a max_len.\n",
    "    \"\"\"\n",
    "    ids = [token_to_id[t] for t in tokens] + [EOS_ID]\n",
    "    assert len(ids) <= max_len, f\"Lunghezza {len(ids)} > {max_len}\"\n",
    "    return ids + [PAD_ID] * (max_len - len(ids))\n",
    "\n",
    "def decode_sequence(token_ids: list[int]) -> str:\n",
    "    \"\"\"\n",
    "    Converte lista di ID in stringa tokenizzata, fermandosi a EOS e ignorando PAD.\n",
    "    Esempio: [ 'c','b','*','e','*','d','b','/','+','EOS',PAD,.. ] -> \"c b * e * d b / +\"\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    for token_id in token_ids:\n",
    "        tok = id_to_token.get(token_id, '?')\n",
    "        if tok == 'EOS':\n",
    "            break\n",
    "        if tok != 'PAD':\n",
    "            tokens.append(tok)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def shift_right(seqs: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Sposta a destra ogni sequenza target per il teacher forcing, \n",
    "    inserendo SOS all'inizio. Shape in input/output: [batch, MAX_LEN].\n",
    "    \"\"\"\n",
    "    shifted = np.zeros_like(seqs)\n",
    "    shifted[:, 1:] = seqs[:, :-1]\n",
    "    shifted[:, 0] = SOS_ID\n",
    "    return shifted\n",
    "\n",
    "def generate_dataset(n: int, max_depth: int = MAX_DEPTH) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Genera n espressioni random, ritorna due array:\n",
    "      - X: [n, MAX_LEN] con ID di infix codificato + EOS + PAD\n",
    "      - Y: [n, MAX_LEN] con ID di postfix codificato + EOS + PAD\n",
    "    \"\"\"\n",
    "    X, Y = [], []\n",
    "    for _ in range(n):\n",
    "        expr = generate_infix_expression(max_depth)\n",
    "        infix = tokenize(expr)\n",
    "        postfix = infix_to_postfix(infix)\n",
    "        X.append(encode(infix))\n",
    "        Y.append(encode(postfix))\n",
    "    return np.array(X, dtype=np.int32), np.array(Y, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c1757",
   "metadata": {},
   "source": [
    "# Attention (Bahdanau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe6cc35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Implementazione di Bahdanau (additive) attention:\n",
    "      score = v^T tanh(W1 · values + W2 · query)\n",
    "      alpha = softmax(score, axis=1)\n",
    "      context = Σ_s alpha_s * values_s\n",
    "    Dove:\n",
    "      - values: intera sequenza di hidden states dell'encoder (shape [batch, seq_len, enc_units])\n",
    "      - query: hidden state corrente del decoder (shape [batch, dec_units])\n",
    "    Ritorna:\n",
    "      - context_vector: [batch, enc_units]\n",
    "      - attention_weights: [batch, seq_len, 1]\n",
    "    \"\"\"\n",
    "    def __init__(self, units: int):\n",
    "        super().__init__()\n",
    "        # W1: proietta encoder_outputs da (enc_units) a (units)\n",
    "        self.W1 = tf.keras.layers.Dense(units)\n",
    "        # W2: proietta decoder_hidden (dec_units) a (units)\n",
    "        self.W2 = tf.keras.layers.Dense(units)\n",
    "        # V: proietta la somma tanh(W1+W2) a un punteggio scalare\n",
    "        self.V  = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, query: tf.Tensor, values: tf.Tensor) -> tuple[tf.Tensor, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        query: [batch, dec_units]\n",
    "        values: [batch, seq_len, enc_units]\n",
    "        \"\"\"\n",
    "        # Aggiungi time axis per query: [batch, 1, dec_units]\n",
    "        hidden_with_time_axis = tf.expand_dims(query, axis=1)\n",
    "\n",
    "        # Calcolo score: shape -> [batch, seq_len, 1]\n",
    "        #   score = V(tanh(W1(values) + W2(query)))\n",
    "        score = self.V(\n",
    "            tf.nn.tanh(\n",
    "                self.W1(values) + self.W2(hidden_with_time_axis)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # attention_weights: softmax su axis=1 (su seq_len)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)  # [batch, seq_len, 1]\n",
    "\n",
    "        # context_vector = Σ_i α_i * values_i\n",
    "        context_vector = attention_weights * values  # broadcasting [batch, seq_len, enc_units]\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)  # [batch, enc_units]\n",
    "\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d1364",
   "metadata": {},
   "source": [
    "# Model SEQ2SEQ with ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d5e81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iperparametri\n",
    "EMB_DIM    = 64      # dimensione embedding (sia encoder che decoder)\n",
    "ENC_UNITS  = 128     # numero di celle in encoder LSTM\n",
    "DEC_UNITS  = 128     # numero di celle in decoder LSTM (coerente con ENC_UNITS)\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# --- 1) Definizione degli Input Layers ---\n",
    "# encoder_input: sequenze infix (shape [batch, MAX_LEN])\n",
    "encoder_inputs = Input(shape=(MAX_LEN,), name='encoder_inputs')\n",
    "\n",
    "# decoder_input: sequenze shiftate (shape [batch, MAX_LEN])\n",
    "decoder_inputs = Input(shape=(MAX_LEN,), name='decoder_inputs')\n",
    "\n",
    "# --- 2) Embedding Layers (encoder e decoder) ---\n",
    "encoder_embedding = Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=EMB_DIM,\n",
    "    embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=SEED),\n",
    "    mask_zero=True,                   # per ignorare PAD\n",
    "    name='encoder_embedding'\n",
    ")(encoder_inputs)  # shape -> [batch, MAX_LEN, EMB_DIM]\n",
    "\n",
    "decoder_embedding = Embedding(\n",
    "    input_dim=VOCAB_SIZE,\n",
    "    output_dim=EMB_DIM,\n",
    "    embeddings_initializer=tf.keras.initializers.GlorotUniform(seed=SEED+1),\n",
    "    mask_zero=True,\n",
    "    name='decoder_embedding'\n",
    ")(decoder_inputs)  # shape -> [batch, MAX_LEN, EMB_DIM]\n",
    "\n",
    "# --- 3) Encoder LSTM (return_sequences + return_state) ---\n",
    "# Usando un LSTM unidirezionale per semplicità; si potrebbero usare Bidirectional\n",
    "encoder_lstm = LSTM(\n",
    "    ENC_UNITS,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer=tf.keras.initializers.GlorotUniform(seed=SEED+2),\n",
    "    name='encoder_lstm'\n",
    ")\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
    "# encoder_outputs: [batch, MAX_LEN, ENC_UNITS]\n",
    "# state_h, state_c: entrambi [batch, ENC_UNITS]\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# --- 4) Decoder LSTM (return_sequences=True per generare tutta la sequenza in training) ---\n",
    "decoder_lstm = LSTM(\n",
    "    DEC_UNITS,\n",
    "    return_sequences=True,\n",
    "    return_state=True,\n",
    "    recurrent_initializer=tf.keras.initializers.GlorotUniform(seed=SEED+3),\n",
    "    name='decoder_lstm'\n",
    ")\n",
    "# Forniamo come initial_state lo stato finale dell'encoder\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
    "# decoder_outputs: [batch, MAX_LEN, DEC_UNITS]\n",
    "\n",
    "# --- 5) Meccanismo di Attention Bahdanau (appliicato ad ogni passo) ---\n",
    "# Creiamo una singola istanza di BahdanauAttention\n",
    "attention_layer = BahdanauAttention(units=DEC_UNITS)\n",
    "\n",
    "# Vogliamo applicare l’attenzione passo per passo su tutta la lunghezza di decoder_outputs.\n",
    "# Tuttavia, Keras non supporta direttamente un loop interno in Functional API.\n",
    "# Quindi possiamo “ricostruire” la parte finale come:\n",
    "#   per ogni passo t in MAX_LEN:\n",
    "#       h_t = decoder_outputs[:, t, :]  # shape [batch, DEC_UNITS]\n",
    "#       context_t, attn_weights_t = attention_layer(h_t, encoder_outputs)\n",
    "#       concat_t = concat([context_t, h_t], axis=-1)  # [batch, DEC_UNITS+ENC_UNITS]\n",
    "#       out_t = Dense(VOCAB_SIZE, activation='softmax')(concat_t)\n",
    "# E poi raccogliere tutti i out_t in una sequenza temporale.\n",
    "#\n",
    "# Con Keras Functional, si può invece fare un _TimeDistributed_ di un “mini-decoder” che\n",
    "# unisce attention e Dense. Tuttavia, per chiarezza, implementiamo un loop via tf.keras.layers.Lambda.\n",
    "\n",
    "def apply_attention_step(args):\n",
    "    \"\"\"\n",
    "    Funzione di utilità per calcolare attenzione e output softmax ad un singolo passo.\n",
    "    args = (decoder_hidden_t, encoder_outputs)\n",
    "      - decoder_hidden_t: [batch, DEC_UNITS]\n",
    "      - encoder_outputs: [batch, MAX_LEN, ENC_UNITS]\n",
    "    Ritorna:\n",
    "      - output_t: [batch, VOCAB_SIZE]\n",
    "    \"\"\"\n",
    "    decoder_hidden_t, encoder_outs = args\n",
    "    # 1) calcolo context vector e att. weights\n",
    "    context_vector, _ = attention_layer(query=decoder_hidden_t, values=encoder_outs)\n",
    "    # 2) concat: [batch, ENC_UNITS] + [batch, DEC_UNITS] = [batch, ENC_UNITS+DEC_UNITS]\n",
    "    concat_vector = K.concatenate([context_vector, decoder_hidden_t], axis=-1)\n",
    "    # 3) Dense -> VOCAB_SIZE con softmax\n",
    "    output_t = Dense(\n",
    "        VOCAB_SIZE,\n",
    "        activation='softmax',\n",
    "        kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED+4)\n",
    "    )(concat_vector)\n",
    "    return output_t  # shape [batch, VOCAB_SIZE]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e34ff8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ATT_UNITS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AdditiveAttention, TimeDistributed, Concatenate, Dense\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1) Proietta encoder_outputs: [batch, MAX_LEN, ENC_UNITS] → [batch, MAX_LEN, ATT_UNITS]\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m proj_enc \u001b[38;5;241m=\u001b[39m Dense(\u001b[43mATT_UNITS\u001b[49m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproj_enc\u001b[39m\u001b[38;5;124m'\u001b[39m)(encoder_outputs)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 2) Proietta decoder_outputs: [batch, MAX_LEN, DEC_UNITS] → [batch, MAX_LEN, ATT_UNITS]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m proj_dec \u001b[38;5;241m=\u001b[39m Dense(ATT_UNITS, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproj_dec\u001b[39m\u001b[38;5;124m'\u001b[39m)(decoder_outputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ATT_UNITS' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# from tensorflow.keras.layers import Lambda\n",
    "from tensorflow.keras.layers import AdditiveAttention, TimeDistributed, Concatenate, Dense\n",
    "\n",
    "# 1) Proietta encoder_outputs: [batch, MAX_LEN, ENC_UNITS] → [batch, MAX_LEN, ATT_UNITS]\n",
    "proj_enc = Dense(ATT_UNITS, name='proj_enc')(encoder_outputs)\n",
    "\n",
    "# 2) Proietta decoder_outputs: [batch, MAX_LEN, DEC_UNITS] → [batch, MAX_LEN, ATT_UNITS]\n",
    "proj_dec = Dense(ATT_UNITS, name='proj_dec')(decoder_outputs)\n",
    "\n",
    "# 3) Usa AdditiveAttention per ottenere tutti i context vectors in parallelo:\n",
    "#    context_seq: [batch, MAX_LEN, ENC_UNITS]\n",
    "attention_layer = AdditiveAttention(name='additive_attention')\n",
    "context_seq = attention_layer([proj_dec, proj_enc], return_attention_scores=False)\n",
    "\n",
    "# 4) Concatena along feature axis → [batch, MAX_LEN, ENC_UNITS + DEC_UNITS]\n",
    "concat_seq = Concatenate(axis=-1, name='concat_context_hidden')([context_seq, decoder_outputs])\n",
    "\n",
    "# 5) Proietta in vocabolario con TimeDistributed: ottieni [batch, MAX_LEN, VOCAB_SIZE]\n",
    "decoder_softmax_outputs = TimeDistributed(\n",
    "    Dense(VOCAB_SIZE, activation='softmax', name='final_dense'),\n",
    "    name='time_distributed_output'\n",
    ")(concat_seq)\n",
    "\n",
    "\n",
    "# # Questo crea un layer che, nel grafo di Keras, applicherà tf.split\n",
    "# dec_hidden_seq_list = Lambda(\n",
    "#     lambda x: tf.split(x, num_or_size_splits=MAX_LEN, axis=1),\n",
    "#     name='split_decoder_outputs'\n",
    "# )(decoder_outputs)\n",
    "\n",
    "# # Nota: dec_hidden_seq_list sarà un TUPLE di MAX_LEN KerasTensor di forma [batch, 1, DEC_UNITS].\n",
    "# # Poi, per ridurre la dimensione 1, usi ancora una Lambda o tf.keras.layers.Reshape.\n",
    "# dec_hidden_seq = [\n",
    "#     Lambda(lambda y: tf.squeeze(y, axis=1), name=f'squeeze_step_{t}')(dec_hidden_seq_list[t])\n",
    "#     for t in range(MAX_LEN)\n",
    "# ]\n",
    "\n",
    "# # Split decoder_outputs lungo l’asse temporale: lista di MAX_LEN tensori [batch, DEC_UNITS]\n",
    "# dec_hidden_seq = tf.split(decoder_outputs, num_or_size_splits=MAX_LEN, axis=1)\n",
    "# # Ogni dec_hidden_seq[i] ha shape [batch, 1, DEC_UNITS], vogliamo ridurlo a [batch, DEC_UNITS]\n",
    "# dec_hidden_seq = [K.squeeze(x, axis=1) for x in dec_hidden_seq]\n",
    "\n",
    "# Per ognuno dei MAX_LEN \"step\", applichiamo apply_attention_step:\n",
    "all_decoder_outputs = []\n",
    "for t in range(MAX_LEN):\n",
    "    # dec_hidden_seq[t]: [batch, DEC_UNITS]\n",
    "    out_t = tf.keras.layers.Lambda(\n",
    "        apply_attention_step,\n",
    "        name=f'attention_step_{t}'\n",
    "    )((dec_hidden_seq[t], encoder_outputs))  # out_t: [batch, VOCAB_SIZE]\n",
    "    all_decoder_outputs.append(out_t)\n",
    "\n",
    "\n",
    "decoder_softmax_outputs = Lambda(\n",
    "    lambda x: tf.stack(x, axis=1),\n",
    "    name='stack_decoder_outputs'\n",
    ")(all_decoder_outputs)\n",
    "# # Ricompattiamo la lista in [batch, MAX_LEN, VOCAB_SIZE]\n",
    "# decoder_softmax_outputs = tf.stack(all_decoder_outputs, axis=1)\n",
    "\n",
    "# --- 6) Modello Keras Finale (Training) ---\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_softmax_outputs)\n",
    "# Compiliamo con loss sparse_categorical_crossentropy (per ID target) e optimizer Adam\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()  # possibilità di verificare il numero totale di parametri (<2M) :contentReference[oaicite:4]{index=4}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e839d3",
   "metadata": {},
   "source": [
    "# Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44eaffe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 15:59:28.969316: E tensorflow/core/framework/node_def_util.cc:676] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "/home/pp26/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['encoder_inputs', 'decoder_inputs']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Lambda.call().\n\n\u001b[1mtf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\u001b[0m\n\nArguments received by Lambda.call():\n  • inputs=('tf.Tensor(shape=(None, 128), dtype=float32)', 'tf.Tensor(shape=(None, 30, 128), dtype=float32)')\n  • mask=('None', 'tf.Tensor(shape=(None, 30), dtype=bool)')\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Eseguo il training (ad esempio 20 epoche)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m EPOCHS \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m---> 24\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_in_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# encoder input + decoder input\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# serve come [batch, MAX_LEN, 1] per sparse_cce\u001b[39;49;00m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec_in_val\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand_dims\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[12], line 89\u001b[0m, in \u001b[0;36mapply_attention_step\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     87\u001b[0m concat_vector \u001b[38;5;241m=\u001b[39m K\u001b[38;5;241m.\u001b[39mconcatenate([context_vector, decoder_hidden_t], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;66;03m# 3) Dense -> VOCAB_SIZE con softmax\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m output_t \u001b[38;5;241m=\u001b[39m \u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43mVOCAB_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msoftmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_initializer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGlorotUniform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSEED\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconcat_vector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_t\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling Lambda.call().\n\n\u001b[1mtf.function only supports singleton tf.Variables created on the first call. Make sure the tf.Variable is only created once or created outside tf.function. See https://www.tensorflow.org/guide/function#creating_tfvariables for more information.\u001b[0m\n\nArguments received by Lambda.call():\n  • inputs=('tf.Tensor(shape=(None, 128), dtype=float32)', 'tf.Tensor(shape=(None, 30, 128), dtype=float32)')\n  • mask=('None', 'tf.Tensor(shape=(None, 30), dtype=bool)')\n  • training=True"
     ]
    }
   ],
   "source": [
    "# Generiamo un dataset fisso (ad esempio 100k esempi) o usiamo un generatore on-the-fly.\n",
    "N_SAMPLES = 100_000\n",
    "X_data, Y_data = generate_dataset(N_SAMPLES, MAX_DEPTH)\n",
    "\n",
    "# Input al decoder per teacher forcing\n",
    "decoder_input_data = shift_right(Y_data)\n",
    "\n",
    "# Poiché la loss è 'sparse_categorical_crossentropy', ci aspettiamo come target gli ID\n",
    "# shape Y_data: [N_SAMPLES, MAX_LEN], ma il softmax output è [batch, MAX_LEN, VOCAB_SIZE],\n",
    "# quindi va bene lasciare Y_data come è (Keras farà il broadcasting).\n",
    "\n",
    "# Divido in train/val (es. 90% train, 10% val)\n",
    "indices = np.arange(N_SAMPLES)\n",
    "np.random.shuffle(indices)\n",
    "split = int(0.9 * N_SAMPLES)\n",
    "train_idx, val_idx = indices[:split], indices[split:]\n",
    "\n",
    "X_train, X_val = X_data[train_idx], X_data[val_idx]\n",
    "dec_in_train, dec_in_val = decoder_input_data[train_idx], decoder_input_data[val_idx]\n",
    "Y_train, Y_val = Y_data[train_idx], Y_data[val_idx]\n",
    "\n",
    "# Eseguo il training (ad esempio 20 epoche)\n",
    "EPOCHS = 20\n",
    "history = model.fit(\n",
    "    [X_train, dec_in_train],        # encoder input + decoder input\n",
    "    np.expand_dims(Y_train, -1),     # serve come [batch, MAX_LEN, 1] per sparse_cce\n",
    "    validation_data=(\n",
    "        [X_val, dec_in_val],\n",
    "        np.expand_dims(Y_val, -1)\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbe1f2d",
   "metadata": {},
   "source": [
    "# Autoregressive inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70469d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per l'inferenza, dobbiamo definire i modelli separate encoder_model e decoder_model,\n",
    "# così da far girare un ciclo passo-passo (greedy) finché non usciamo con EOS.\n",
    "\n",
    "# ==== 1) Modello encoder per inferenza (input -> hidden states) ====\n",
    "encoder_model_inf = Model(encoder_inputs, [encoder_outputs, state_h, state_c])\n",
    "\n",
    "# ==== 2) Modello decoder per inferenza ====\n",
    "# Input placeholder per hidden state e cell state del decoder (prendono ENC_UNITS)\n",
    "decoder_state_input_h = Input(shape=(DEC_UNITS,), name='dec_state_h')\n",
    "decoder_state_input_c = Input(shape=(DEC_UNITS,), name='dec_state_c')\n",
    "# Input placeholder per encoder_outputs (per l’attenzione) [batch, MAX_LEN, ENC_UNITS]\n",
    "encoder_outputs_inf = Input(shape=(MAX_LEN, ENC_UNITS), name='enc_outputs_inf')\n",
    "\n",
    "# Embedding del token di input corrente (shape [batch=1, 1]) in inference\n",
    "dec_single_input = Input(shape=(1,), name='dec_single_input')  # st(1) per inferenza\n",
    "dec_single_emb = decoder_embedding.layer(dec_single_input)      # usiamo lo stesso embedding\n",
    "\n",
    "# LSTM del decoder ad un singolo passo, con hidden iniziali\n",
    "dec_lstm_inf = decoder_lstm  # usiamo lo stesso layer addestrato\n",
    "# Riduci dec_single_emb (che è [batch, 1, EMB_DIM]) in [batch, 1, EMB_DIM],\n",
    "# già corretto shape.\n",
    "dec_outputs, dec_state_h_new, dec_state_c_new = dec_lstm_inf(\n",
    "    dec_single_emb, initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    ")  # dec_outputs: [batch, 1, DEC_UNITS]\n",
    "\n",
    "# Rimuovo la dimensione temporale: [batch, DEC_UNITS]\n",
    "dec_hidden_t = K.squeeze(dec_outputs, axis=1)\n",
    "\n",
    "# Applico attention: query=dec_hidden_t, values=encoder_outputs_inf\n",
    "context_vector_inf, _ = attention_layer(dec_hidden_t, encoder_outputs_inf)\n",
    "concat_vector_inf = K.concatenate([context_vector_inf, dec_hidden_t], axis=-1)\n",
    "# Softmax output\n",
    "dec_pred = Dense(\n",
    "    VOCAB_SIZE,\n",
    "    activation='softmax',\n",
    "    kernel_initializer=tf.keras.initializers.GlorotUniform(seed=SEED+4)\n",
    ")(concat_vector_inf)  # [batch, VOCAB_SIZE]\n",
    "\n",
    "# Definisco il modello decoder_inferenza che, dato:\n",
    "#  - dec_single_input: token corrente [batch,1]\n",
    "#  - encoder_outputs_inf: [batch,MAX_LEN,ENC_UNITS]\n",
    "#  - dec_state_input_h, dec_state_input_c: [batch,DEC_UNITS]\n",
    "# produce:\n",
    "#  - dec_pred: [batch, VOCAB_SIZE] predisposto all'argmax\n",
    "#  - dec_state_h_new, dec_state_c_new (nuovi state)\n",
    "decoder_model_inf = Model(\n",
    "    [dec_single_input, encoder_outputs_inf, decoder_state_input_h, decoder_state_input_c],\n",
    "    [dec_pred, dec_state_h_new, dec_state_c_new]\n",
    ")\n",
    "\n",
    "# ==== 3) Funzione di decodifica greedy in inferenza ====\n",
    "def decode_sequence_inference(input_seq: np.ndarray) -> list[int]:\n",
    "    \"\"\"\n",
    "    Data una singola sequenza infix (shape [1, MAX_LEN]), genera la corrispondente \n",
    "    postfix autoregressivamente fino a EOS o lunghezza MAX_LEN. Ritorna lista di ID.\n",
    "    \"\"\"\n",
    "    # 1) Ottengo encoder_outputs e stati iniziali\n",
    "    enc_outs, enc_h, enc_c = encoder_model_inf.predict(input_seq)\n",
    "    # inizializzo decoder con SOS\n",
    "    target_seq = np.array([[SOS_ID]], dtype=np.int32)\n",
    "    dec_h, dec_c = enc_h, enc_c\n",
    "\n",
    "    output_ids = []\n",
    "    for _ in range(MAX_LEN):\n",
    "        # 2) Chiama decoder one-step\n",
    "        preds, dec_h, dec_c = decoder_model_inf.predict([target_seq, enc_outs, dec_h, dec_c])\n",
    "        # preds: [1, VOCAB_SIZE]\n",
    "        sampled_id = np.argmax(preds[0])\n",
    "        # Se arrivo a EOS, fermo\n",
    "        if sampled_id == EOS_ID:\n",
    "            break\n",
    "        output_ids.append(sampled_id)\n",
    "        # preparo input per passo successivo: shape [1,1]\n",
    "        target_seq = np.array([[sampled_id]], dtype=np.int32)\n",
    "\n",
    "    return output_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d6c9a",
   "metadata": {},
   "source": [
    "#  Metric (Prefix Accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_accuracy(y_true_ids: list[int], y_pred_ids: list[int]) -> float:\n",
    "    \"\"\"\n",
    "    Calcola la prefix_accuracy tra due sequence di ID (senza pad).\n",
    "    Ritorna lunghezza del prefisso identico / max(len(y_true), len(y_pred)).\n",
    "    \"\"\"\n",
    "    # Tronco alla comparsa di EOS (se presente) – ma qui assume che le sequenze non contengano EOS        \n",
    "    # Confronto elemento per elemento\n",
    "    match_len = 0\n",
    "    L_true = len(y_true_ids)\n",
    "    L_pred = len(y_pred_ids)\n",
    "    L_max = max(L_true, L_pred)\n",
    "    for i in range(min(L_true, L_pred)):\n",
    "        if y_true_ids[i] == y_pred_ids[i]:\n",
    "            match_len += 1\n",
    "        else:\n",
    "            break\n",
    "    return match_len / L_max if L_max > 0 else 1.0\n",
    "\n",
    "# Esempio di utilizzo:\n",
    "i = np.random.randint(len(X_val))\n",
    "input_seq = X_val[i : i + 1]  # shape [1, MAX_LEN]\n",
    "true_postfix = Y_val[i]\n",
    "pred_ids = decode_sequence_inference(input_seq)\n",
    "print(\"Infix   : \", decode_sequence(input_seq[0]))\n",
    "print(\"TP (true postfix) : \", decode_sequence(true_postfix))\n",
    "print(\"Pred    : \", decode_sequence(pred_ids))\n",
    "print(\"PrefixAcc        : \", prefix_accuracy(\n",
    "    # rimuovo pad/EOS in true_postfix\n",
    "    [tok for tok in true_postfix if tok not in (PAD_ID, EOS_ID)],\n",
    "    pred_ids\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
